{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System\n",
    "\n",
    "The goal of this document is to provide a way to recommend treatments to users.  For each condition, we can see what treatments have worked for other patients.  We can also go one step further and say, if Treatment/Tag A has worked for you, then other people who have had success with Treatment/Tag A have also had success with Treatment/Tag B.\n",
    "\n",
    "The same will also be possible in reverse.  Some Treatments/Tags may cause Conditions/Symptoms to worsen, and we may be able to recommend against those Treatments/Tags.\n",
    "\n",
    "In order to say that a treatment is working, we need a measure of that.  There are a few strategies for doing that, so it's handled in a separate notebook \"treatment_effectiveness\".\n",
    "\n",
    "### Filter Type\n",
    "\n",
    "We will use a collaborative filter to make our recommendations, but there are two different types that we need to consider.  Item based filtering will form groups of associated items(in our case, an item is a treatment/tag), and recommend people who have good results with items in that set to other items in that set.  User based filtering will try to form groups of users that have success with similar items, and make recommendations based on what items work well for that group.  We will of course figure out which one is best for our situation by trying both.\n",
    "\n",
    "VERSION INFO : The user-based recommender below uses profile information which was added in the 083016 version of the datafile, use that one or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  age     sex country         condition     treatment  before_value  \\\n",
      "0       20   49    male      US           Fatigue      Provigil      1.750000   \n",
      "1       20   49    male      US  major somnolence      Provigil      2.000000   \n",
      "2       20   49    male      US        sleepiness      Provigil      1.750000   \n",
      "3       52   44  female      US           Allergy  Escitalopram      1.083333   \n",
      "4       52   44  female      US           Allergy     Magnesium      0.800000   \n",
      "\n",
      "   after_value  effectiveness  \n",
      "0     1.444444       0.305556  \n",
      "1     1.333333       0.666667  \n",
      "2     1.777778      -0.027778  \n",
      "3     0.500000       0.583333  \n",
      "4     0.333333       0.466667  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"effectiveness_083016.csv\")\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Based Collaborative Filtering\n",
    "\n",
    "We will start by trying to predict a single treatment for a single condition, and see how that goes.  Since Depression is common, we will start with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stressed         45\n",
      "tired            42\n",
      "ate breakfast    35\n",
      "good sleep       31\n",
      "period           22\n",
      "happy            20\n",
      "had sex          19\n",
      "alcohol          18\n",
      "walked           17\n",
      "Anxious          14\n",
      "exercise         14\n",
      "travel           13\n",
      "Headache         13\n",
      "Superlong nap    13\n",
      "bad sleep        13\n",
      "Name: treatment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df[df['condition'] == \"Depression\"]['treatment'].value_counts().head(15)\n",
    "#print df[(df['treatment'] == \"good sleep\") & (df['condition'] == \"Depression\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OK, so we are hurting for samples of specific treatments.  Let's go with \"good sleep\" as it has the most samples out of the tags which sound like they might help with depression.\n",
    "We will hold a few users back who have reported good sleep while suffering from depression.  Our first goal will be to use the rest of the users to create a model which can accurately predict the effectiveness for the test users.\n",
    "\n",
    "First up we will make a recommendation on \"good sleep\" by finding which other treatment/tag is most correlated to it.  Correlation makes a great distance measure because it gives a p-value which can be used to assess how significant the distance measures are. Pearson correlation is a measure of how much a variable changes relative to another variable, divided by how much they change independently.  This will help us accomodate the fact that not all users will rate their symptoms the same.  Presumably, some users will consistently rate their symptoms as being worse than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humid': 0.93698444082858501, 'neck ache': 1.0, 'Stayed at home': 1.0, 'Upset stomach': 0.54524358134172757, 'cold': -1.0, 'ate breakfast': 0.4853091842706253, 'middleschmertz': -0.99999999999999978, 'busy': 0.72247406517454238, 'family': 1.0, 'ovarian cramps': -1.0, \"can't sleep\": 1.0, 'paranoia': 1.0, 'sugar': 1.0, 'ovulating': -1.0, 'dairy': 0.12883874324150038, 'shoulder pain': 1.0, ' toothache': 0.99999999999999989, 'distraction': 1.0, 'good day': -1.0, 'right knee weakness': 1.0, 'Adam over': -1.0, 'chest pain': 1.0, 'sore legs': 1.0, 'nausea': 1.0, 'Benadryl': 0.99999999999999989, 'congested': -1.0, 'bad sleep': 0.67050082708071557, 'Day off': -0.99999999999999978, 'Marijuana': -1.0, 'anxious': 0.99999999999999989, 'doctor appointment': 1.0, 'had therapy': 1.0, 'overslept': 1.0, 'Went to work': 0.4767596381184977, 'neck pain': 0.99999999999999989, 'unproductive': 1.0, 'tired': 0.52507391318031171, 'household chores': 1.0, 'Period': 1.0, 'fast food': 1.0, 'napped': 0.66666669027453895, 'worried': 0.27678467024181136, 'Bad day': 1.0, 'bleh': -0.99999999999999989, 'Headache': -0.97094281420979733, 'gross': 1.0, 'stressful': -0.99999999999999978, 'alcohol': 0.77133809010216081, 'poor sleep': 0.35856217050507139, 'bad dreams': 0.99999999999999989, 'convention day': -1.0, 'Dexedrine': nan, 'jaw ache': 1.0, 'neckache': 0.99999999999999989, 'exercise': 0.95317934723746411, 'Superlong nap': 0.94245864542829683, 'brain fog': 1.0, 'confusion': -0.99999999999999978, 'Work': -1.0, 'not enough sleep': -0.02179966149063696, 'Poor sleep quality': 1.0, 'packing': 0.99078265693450296, 'congestion': 1.0, 'active': 0.73643779613983795, 'hip pain': 1.0, 'Rainy Day': 0.99999999999999989, 'shabbat': 0.27754911201655602, 'slept in': 0.23994516925497736, 'slept badly': -0.99999999999999989, 'Cymbalta': -0.75385567941461651, 'stressed': 0.91727757410990507, 'ibs issues': 1.0, 'chills': -1.0, 'little sleep': 1.0, 'suicidal ideation': 1.0, 'Yoga': 1.0, 'danced': 0.90159691762341487, 'hiking': 0.63641821515071817, 'period': 0.88387381303061585, 'had sex': 0.81015298560900206, 'dental work': 1.0, 'cramps': -0.99999999999999978, 'dentist visit': 1.0, 'lots of walking': 1.0, 'off my meds': -1.0, 'difficulty breathing': 1.0, 'exhausted': 0.99999999999999989, 'sleepover': -1.0, 'so much pain': -1.0, 'calm': 0.99999999999999989, 'tooth pain': -1.0, 'N-acetyl cysteine': 1.0, 'no work': 0.33773659842130571, 'slept late': 1.0, 'hyperventilating': 1.0, 'headache': 1.0, 'Car travel': 1.0, 'loss of appetite': -1.0, 'gluten': 1.0, 'counselling': -1.0, 'relaxed': 0.27018165483380235, 'sleep paralysis': -1.0, 'job interview': -1.0, 'gum pain': 1.0, 'leg cramps': 1.0, 'worked': -0.99999999999999989, 'cramping': -1.0, 'hot and humid': 1.0, 'anxiety': -0.67908157363105204, 'late night': 1.0, 'gassy': 1.0, 'depressed': -1.0, 'work': -0.028502515518841847, 'Alcohol previous night': -1.0, 'black dogs': -0.99999999999999989, 'Ibuprofen': -1.0, 'body aches': -1.0, 'happy': 0.28376858082346212, 'menstruation': 0.77075261423951758, 'jet lag': 1.0, 'left the house': 1.0, 'No Exercise': -1.0, 'Acetaminophen': -1.0, 'very tired': 1.0, 'sad': 1.0, 'cleaned': 1.0, 'food poisoning': 0.99999999999999978, 'junk food': 1.0, 'Good day': 0.078845643646479585, 'trouble getting up': 0.74428827216768367, 'productive': 0.88250757359188881, 'yoga': 1.0, 'choking': -0.99999999999999989, 'migraine': 0.85801944407584452, 'travel': 0.94284913443910523, 'nap': 1.0, 'sick': 0.97035396428507192, 'panic attack': 0.99999999999999978, 'feet tingly': 1.0, 'itchy eyes': 1.0, 'pain': 1.0, 'shoulder ache': 1.0, 'back pain': 0.98246264138964201, 'menstruating': 0.99999999999999978, 'Daytime nap': 0.99999999999999989, 'fatigue': -1.0, 'cleaning': 0.99999999999999978, 'no breakfast': 1.0, 'Mebeverine': 0.99999999999999978, 'good tired': 1.0, 'long day': 1.0, 'Anxious': 0.90960676992380352, 'shortness of breath': -1.0, 'hungry': -0.99999999999999989, 'walked': 0.5243943388209833, 'cooked': 1.0, 'forgot one dose of meds': 0.65271861757395488, 'tooth ache': -1.0, 'Irritable': -0.75869499179916911}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "#set gives us a list of all distinct users, as well as a shuffle\n",
    "good_sleep_users = list(set(df[(df['treatment'] == \"good sleep\") & (df['condition'] == \"Depression\")]['user_id']))\n",
    "test_users = good_sleep_users[:6]  #grab %20 of users for testing\n",
    "train_users = good_sleep_users[6:]\n",
    "train_rows = df[df['user_id'].isin(train_users)]\n",
    "test_rows = df[df['user_id'].isin(test_users)]\n",
    "\n",
    "#just going to abstract this now in case we need it later\n",
    "#finds the pearson correlation between the specified treatment, and all of the treatments used to in conjunction with the specified condition\n",
    "def correlate_treatments(train_df, treatment, condition):\n",
    "    affected_rows = train_df[train_df['condition'] == condition]\n",
    "    other_treatments = list(set(affected_rows[affected_rows['treatment'] != treatment]['treatment']))\n",
    "    treatment_correlations = {}\n",
    "    for treatment2 in other_treatments:\n",
    "        users_with_treatment = list(set(affected_rows[affected_rows['treatment'] == treatment2]['user_id']))\n",
    "        treatment1_values = affected_rows[(affected_rows['user_id'].isin(users_with_treatment)) & (affected_rows['treatment'] == treatment)]['effectiveness']\n",
    "        treatment2_values = affected_rows[(affected_rows['user_id'].isin(users_with_treatment)) & (affected_rows['treatment'] == treatment2)]['effectiveness']\n",
    "        if len(treatment1_values) > 1 :\n",
    "            correlation = pearsonr(treatment1_values, treatment2_values)[0]\n",
    "            treatment_correlations[treatment2] = correlation\n",
    "    return treatment_correlations\n",
    "        \n",
    "treatment_correlations = correlate_treatments(train_rows, 'good sleep', 'Depression')\n",
    "print treatment_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that the number of correlations we've learned between \"good sleep\" and other treatments for depression is very low.  \n",
    "The reason for this is that if we look at users that have tried \"good sleep\" and any other tag, they are almost always the only user that has tried that combination.  Which leaves us comparing single values, which Pearson can't help us with.  We can also see a lot of correlations that are basically 1 or -1, which are usually occurring when we have just two users with the same tags.\n",
    "\n",
    "This may still be the best way to build the recommender system, but the volume of data would need to increase, probably by orders of magnitude.\n",
    "\n",
    "We can still try a different distance measure, so let's try the cosine similarity.  This way we can still get the distance between two points.  Whether those distances will be useful remains to be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cosine_distances(train_df, treatment, condition):\n",
    "    affected_rows = train_df[train_df['condition'] == condition]\n",
    "    other_treatments = list(set(affected_rows[affected_rows['treatment'] != treatment]['treatment']))\n",
    "    treatment_correlations = {}\n",
    "    for treatment2 in other_treatments:\n",
    "        users_with_treatment = list(set(affected_rows[affected_rows['treatment'] == treatment2]['user_id']))\n",
    "        treatment1_values = affected_rows[(affected_rows['user_id'].isin(users_with_treatment)) & (affected_rows['treatment'] == treatment)]['effectiveness']\n",
    "        treatment2_values = affected_rows[(affected_rows['user_id'].isin(users_with_treatment)) & (affected_rows['treatment'] == treatment2)]['effectiveness']\n",
    "        if not np.isnan(treatment2_values).any():\n",
    "            cos_distance = distance.cosine(treatment1_values, treatment2_values)\n",
    "            treatment_correlations[treatment2] = cos_distance\n",
    "    return treatment_correlations\n",
    "\n",
    "treatment_correlations = cosine_distances(train_rows, 'good sleep', 'Depression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is a bit long to print out.  There are a lot of tags, but a lot of them have a distance of 0.   Still, if one of the 0 length tags is found in our test set these entries might be useful.  It's the equivalent of saying \"this treatment worked this well for this one other person\".  So maybe better than nothing.\n",
    "\n",
    "I will now perform a test and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value most correlated to good sleep was happy with value 0.563930772324\n",
      "value most correlated to good sleep was tired with value 0.625685887011\n",
      "value most correlated to good sleep was couch potato with value 2.0\n",
      "value most correlated to good sleep was happy with value 0.563930772324\n",
      "value most correlated to good sleep was cleaning with value 0.366664212496\n",
      "value most correlated to good sleep was walked with value 0.385388498727\n",
      "predicted value is 0.75 real value is 0.222222222222\n",
      "predicted value is 0.0 real value is -1.0\n",
      "predicted value is -0.0454545454545 real value is 0.2\n",
      "predicted value is -0.8 real value is -0.8\n",
      "predicted value is -0.666666666667 real value is -1.41666666667\n",
      "predicted value is 1.16666666667 real value is 1.16666666667\n",
      "r2 accuracy score 0.591630696131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#takes all of the treatments that a user has tried, and finds which one is most correlated to \"good sleep\"\n",
    "def predict_effectiveness(x):\n",
    "    highestCorrelationValue = 0\n",
    "    highestCorrelationKey = \"\"\n",
    "    highestCorrelationEffectiveness = 0\n",
    "    for value in x:\n",
    "        if value in treatment_correlations.keys():\n",
    "            if treatment_correlations[value] > highestCorrelationValue:\n",
    "                highestCorrelationValue = treatment_correlations[value]\n",
    "                highestCorrelationKey = value\n",
    "                highestCorrelationEffectiveness = treatment_correlations\n",
    "    print \"value most correlated to good sleep was \" + highestCorrelationKey + \" with value \" + str(highestCorrelationValue)\n",
    "    return str(highestCorrelationValue) + \",\" + highestCorrelationKey\n",
    "\n",
    "test_depression_rows = test_rows[test_rows['condition'] == 'Depression']\n",
    "test_depression_rows['closest_correlation'] = test_depression_rows.groupby('user_id')['treatment'].transform(predict_effectiveness)\n",
    "real_values = []\n",
    "predicted_values = []\n",
    "for user in test_users:\n",
    "    corr_value,corr_key = test_depression_rows[(test_depression_rows['user_id'] == user) & (test_depression_rows['treatment'] == 'good sleep')]['closest_correlation'].values[0].split(',')\n",
    "    predicted_value = test_depression_rows[(test_depression_rows['user_id'] == user) & (test_depression_rows['treatment'] == corr_key)]['effectiveness'].values[0]\n",
    "    real_value = test_depression_rows[(test_depression_rows['user_id'] == user)]['effectiveness'].values[0]\n",
    "    print \"predicted value is \" + str(predicted_value) + \" real value is \" + str(real_value)\n",
    "    real_values.append(real_value)\n",
    "    predicted_values.append(predicted_value)\n",
    "print \"r2 accuracy score \" + str(r2_score(real_values, predicted_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Based Collaborative Filtering with KMeans\n",
    "\n",
    "Initial results on the item based filter are not exactly ready from production, but still encouraging.  It's showing some predictive power, and on very little data.  Lets not get to excited though, due to the very small test set size.\n",
    "\n",
    "Lets try the other approach.  This way of building a collaborative filter uses that strategy of finding a user's neighbours.  This can be done either with K-Means or with K-Nearest Neighbours.  Our clustering analysis shows that either of these options will make good candidates, so we are going to have to try both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pivot_to_per_user(df):\n",
    "    user_df = df.pivot_table(index='user_id',columns='treatment',values='effectiveness').reset_index()\n",
    "    return user_df.fillna(0)\n",
    "\n",
    "all_good_sleep = pivot_to_per_user(df[df['user_id'].isin(good_sleep_users)])\n",
    "\n",
    "#now add in one-hotted profile info\n",
    "#since we are trying to find users most similar to each other age, sex, and location might prove helpful\n",
    "profile = df[['user_id','sex','age','country']].drop_duplicates()\n",
    "all_good_sleep = pd.merge(all_good_sleep, profile, on='user_id', how='left')\n",
    "sex = pd.get_dummies(all_good_sleep['sex_y'])\n",
    "country = pd.get_dummies(all_good_sleep['country'])\n",
    "all_good_sleep = pd.concat([all_good_sleep, sex], axis=1)\n",
    "all_good_sleep = pd.concat([all_good_sleep, country], axis=1)\n",
    "all_good_sleep = all_good_sleep.drop('user_id', axis=1)\n",
    "all_good_sleep = all_good_sleep.drop('sex_y', axis=1)\n",
    "all_good_sleep = all_good_sleep.drop('country', axis=1)\n",
    "all_good_sleep['age'] = all_good_sleep['age'].fillna(np.mean(all_good_sleep['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 accuracy score with num_clusters=3 pca_components=50 : -0.619685461877\n",
      "r2 accuracy score with num_clusters=3 pca_components=100 : -0.619685461877\n",
      "r2 accuracy score with num_clusters=3 pca_components=200 : -0.619685461877\n",
      "r2 accuracy score with num_clusters=5 pca_components=50 : -0.595061946057\n",
      "r2 accuracy score with num_clusters=5 pca_components=100 : -0.677492200554\n",
      "r2 accuracy score with num_clusters=5 pca_components=200 : -0.622317158881\n",
      "r2 accuracy score with num_clusters=10 pca_components=50 : -0.724690080404\n",
      "r2 accuracy score with num_clusters=10 pca_components=100 : -0.811948469328\n",
      "r2 accuracy score with num_clusters=10 pca_components=200 : -0.829812694566\n",
      "r2 accuracy score with num_clusters=15 pca_components=50 : -0.545811060786\n",
      "r2 accuracy score with num_clusters=15 pca_components=100 : -0.54021837861\n",
      "r2 accuracy score with num_clusters=15 pca_components=200 : -0.54021837861\n",
      "r2 accuracy score with num_clusters=20 pca_components=50 : -0.249063845758\n",
      "r2 accuracy score with num_clusters=20 pca_components=100 : -0.249063845758\n",
      "r2 accuracy score with num_clusters=20 pca_components=200 : -0.249063845758\n",
      "r2 accuracy score with num_clusters=25 pca_components=50 : -0.443812826389\n",
      "r2 accuracy score with num_clusters=25 pca_components=100 : -0.443812826389\n",
      "r2 accuracy score with num_clusters=25 pca_components=200 : -0.443812826389\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#PCA, then K-means, then find the mean effectiveness for the target treatment for each cluster\n",
    "#data is the df containing both your train and test sets\n",
    "#split is how many users you want split into the test set\n",
    "#pca_components and num_clusters are hyperparameters\n",
    "#treatment is the target treatment\n",
    "def find_cluster_effectiveness(data, split, pca_components, num_clusters, treatment):\n",
    "    #applying PCA because the number of features in the pivoted dataset is very high\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    pca.fit(data)\n",
    "    fit_data = pca.transform(all_good_sleep)\n",
    "    train = fit_data[split:]\n",
    "    test = fit_data[:split]\n",
    "    test_df = all_good_sleep.iloc[:split,:] #splitting on same lines so I can look these users up at test time\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(train)\n",
    "    pred_train = kmeans.predict(train)\n",
    "    pred_test = kmeans.predict(test)\n",
    "\n",
    "    #now look up which cluster each test sample falls into, and take the mean \"good_sleep\" effectiveness for that cluster\n",
    "    cluster_effectivness = []\n",
    "    for i in range(num_clusters):\n",
    "        rows_in_cluster = all_good_sleep.iloc[np.where(pred_train == i)[0],:][treatment]\n",
    "        cluster_effectivness.append(np.mean(rows_in_cluster))\n",
    "\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    for i in range(len(pred_test)):\n",
    "        predicted.append(cluster_effectivness[pred_test[i]])\n",
    "        actual.append(test_df.iloc[i,:][treatment])\n",
    "        #print \"predicted value is \" + str(cluster_effectivness[pred_test[i]]) + \" actual value is \" + str(test_df.iloc[i,:][treatment])\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    return r2\n",
    "\n",
    "#search to find best PCA components and num clusters\n",
    "num_clusters_list = [3, 5, 10, 15,20,25]\n",
    "pca_components_list = [50, 100, 200]\n",
    "for num_clusters in num_clusters_list:\n",
    "    for pca_components in pca_components_list:\n",
    "        r2 = find_cluster_effectiveness(all_good_sleep, 6, pca_components,num_clusters,\"good sleep\")\n",
    "        print \"r2 accuracy score with num_clusters=\" + str(num_clusters) + \" pca_components=\" +str(pca_components) + \" : \"+ str(r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative scores, this model is performing worse than just guessing the mean of good sleep effectiveness.  Brutal.\n",
    "\n",
    "\n",
    "### User Based Collaborative Filtering with KMeans\n",
    "\n",
    "This is finding the K users that are most like the test user, and taking the mean of their target values.  Fortunately for me, this is exactly what scikit-learn's Nearest Neighbour Regressor does.  And this time I'll be able to use exhaustive grid search to find my parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 5, 'weights': 'uniform'}\n",
      "[-0.01175926 -0.12170198  0.04530992  0.23189434 -0.35776607 -0.06608979\n",
      " -0.02884755]\n",
      "2     0.070833\n",
      "29   -0.380952\n",
      "13    0.125000\n",
      "10    1.250000\n",
      "27    0.711111\n",
      "25    0.297259\n",
      "22   -0.191139\n",
      "Name: good sleep, dtype: float64\n",
      "-0.304938246319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import grid_search\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#using same data structure as kmeans solution\n",
    "X = all_good_sleep.drop('good sleep', axis=1)\n",
    "y = all_good_sleep['good sleep']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "parameters = {'weights':('uniform', 'distance'), 'n_neighbors':[1,2,3,4,5,6,7,8]}\n",
    "nn = KNeighborsRegressor(1)\n",
    "clf = grid_search.GridSearchCV(nn, parameters)  #scorer defaults to r2\n",
    "clf.fit(X_train, y_train)\n",
    "print clf.best_params_\n",
    "pred = clf.predict(X_test)\n",
    "print pred\n",
    "print y_test\n",
    "print r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch.  We might be able to improve that with PCA.  But given the extremely low score, it doesn't seem like a valuable use of time to continue this line of thinking.\n",
    "\n",
    "\n",
    "I'm uncertain, but I believe the reason for the user based filtering not working well may have to do with having so many features that are unrelated to the predicted value.  It is even likely that almost all of the features that I'm using to describe a user are totally irrelevant to the value that I'm trying to predict.  The item-based method does not suffers from because it finds the most relevant feature (by cosine distance) and discards the rest.  So some further investigation may involve trying to solve this problem.\n",
    "\n",
    "\n",
    "### Wrap-up\n",
    "\n",
    "The item based recommender is greatly outperforming both types of user based recommenders.  For the time being I'll be proceeding with building that out.  It should be noted though, that as this dataset grows in both breadth and depth, we should revisit this notebook.  Just drop in a new CSV and run each section and see how the r2 scores change. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
