{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System\n",
    "\n",
    "The goal of this document is to provide a way to recommend treatments to users.  For each condition, we can see what treatments have worked for other patients.  We can also go one step further and say, if Treatment/Tag A has worked for you, then other people who have had success with Treatment/Tag A have also had success with Treatment/Tag B.\n",
    "\n",
    "The same will also be possible in reverse.  Some Treatments/Tags may cause Conditions/Symptoms to worsen, and we may be able to recommend against those Treatments/Tags.\n",
    "\n",
    "In order to say that a treatment is working, we need a measure of that.  There are a few strategies for doing that, so it's handled in a separate notebook \"treatment_effectiveness\".\n",
    "\n",
    "### Filter Type\n",
    "\n",
    "We will use a collaborative filter to make our recommendations, but there are two different types that we need to consider.  Item based filtering will form groups of associated items(in our case, an item is a treatment/tag), and recommend people who have good results with items in that set to other items in that set.  User based filtering will try to form groups of users that have success with similar items, and make recommendations based on what items work well for that group.  We will of course figure out which one is best for our situation by trying both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id         condition     treatment  before_value  after_value  \\\n",
      "0       20           Fatigue      Provigil      1.750000     1.444444   \n",
      "1       20  major somnolence      Provigil      2.000000     1.333333   \n",
      "2       20        sleepiness      Provigil      1.750000     1.777778   \n",
      "3       52           Allergy  Escitalopram      1.083333     0.500000   \n",
      "4       52           Allergy     Magnesium      0.800000     0.333333   \n",
      "\n",
      "   effectiveness  \n",
      "0       0.305556  \n",
      "1       0.666667  \n",
      "2      -0.027778  \n",
      "3       0.583333  \n",
      "4       0.466667  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"effectiveness_082516.csv\")\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Based Collaborative Filtering\n",
    "\n",
    "We will start by trying to predict a single treatment for a single condition, and see how that goes.  Since Depression is common, we will start with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stressed         44\n",
      "tired            41\n",
      "ate breakfast    35\n",
      "good sleep       30\n",
      "period           21\n",
      "had sex          20\n",
      "happy            20\n",
      "alcohol          18\n",
      "walked           16\n",
      "exercise         14\n",
      "Anxious          14\n",
      "Ibuprofen        13\n",
      "Superlong nap    13\n",
      "travel           12\n",
      "poor sleep       12\n",
      "Name: treatment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df[df['condition'] == \"Depression\"]['treatment'].value_counts().head(15)\n",
    "#print df[(df['treatment'] == \"good sleep\") & (df['condition'] == \"Depression\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OK, so we are hurting for samples of specific treatments.  Let's go with \"good sleep\" as it has the most samples out of the tags which sound like they might help with depression.\n",
    "We will hold a few users back who have reported good sleep while suffering from depression.  Our first goal will be to use the rest of the users to create a model which can accurately predict the effectiveness for the test users.\n",
    "\n",
    "First up we will make a recommendation on \"good sleep\" by finding which other treatment/tag is most correlated to it.  Correlation makes a great distance measure because it gives a p-value which can be used to assess how significant the distance measures are. Pearson correlation is a measure of how much a variable changes relative to another variable, divided by how much they change independently.  This will help us accomodate the fact that not all users will rate their symptoms the same.  Presumably, some users will consistently rate their symptoms as being worse than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'humid': 0.93698444082858501, 'neck ache': 1.0, 'Stayed at home': 1.0, 'Upset stomach': 0.54524358134172757, 'cold': -0.99999999999999978, 'ate breakfast': 0.4853091842706253, 'middleschmertz': -0.99999999999999978, 'busy': 0.72247406517454238, 'family': 1.0, 'ovarian cramps': -1.0, \"can't sleep\": 1.0, 'paranoia': 1.0, 'sugar': 1.0, 'ovulating': -1.0, 'dairy': 0.12883874324150038, 'shoulder pain': 1.0, ' toothache': 0.99999999999999989, 'distraction': 1.0, 'good day': -1.0, 'right knee weakness': 1.0, 'Adam over': -1.0, 'chest pain': 1.0, 'sore legs': 1.0, 'nausea': 1.0, 'Benadryl': 0.99999999999999989, 'congested': -1.0, 'bad sleep': 0.67050082708071557, 'Day off': -0.99999999999999978, 'Marijuana': -1.0, 'anxious': 1.0, 'doctor appointment': 1.0, 'had therapy': 1.0, 'overslept': 1.0, 'Went to work': 0.4767596381184977, 'neck pain': 0.99999999999999989, 'unproductive': 1.0, 'tired': 0.51498934898568571, 'household chores': 1.0, 'Period': 1.0, 'fast food': 1.0, 'napped': 0.66666669027453895, 'worried': 0.27926561912695863, 'Bad day': 1.0, 'bleh': -0.99999999999999989, 'Headache': -0.97094281420979733, 'gross': 1.0, 'stressful': -1.0, 'alcohol': 0.77133809010216081, 'poor sleep': 0.35856217050507139, 'bad dreams': 0.99999999999999989, 'convention day': -1.0, 'Dexedrine': nan, 'jaw ache': 1.0, 'neckache': 0.99999999999999989, 'exercise': 0.95317934723746411, 'Superlong nap': 0.94245864542829683, 'brain fog': 1.0, 'confusion': -0.99999999999999978, 'Work': -1.0, 'not enough sleep': -0.02179966149063696, 'Poor sleep quality': 1.0, 'packing': 0.99078265693450296, 'congestion': 1.0, 'active': 0.73643779613983795, 'hip pain': 1.0, 'Rainy Day': 0.99999999999999989, 'shabbat': 0.27754911201655602, 'slept in': 0.23994516925497736, 'slept badly': -0.99999999999999989, 'Cymbalta': -0.75385567941461651, 'stressed': 0.91665741236383402, 'ibs issues': 1.0, 'chills': -1.0, 'little sleep': 1.0, 'suicidal ideation': 1.0, 'Yoga': 1.0, 'danced': 0.90159691762341487, 'hiking': 0.63641821515071817, 'period': 0.87955597106686623, 'had sex': 0.81015298560900206, 'dental work': 1.0, 'cramps': -0.99999999999999978, 'dentist visit': 1.0, 'lots of walking': 1.0, 'off my meds': -1.0, 'difficulty breathing': 1.0, 'exhausted': 0.99999999999999989, 'sleepover': -1.0, 'so much pain': -1.0, 'calm': 0.99999999999999989, 'tooth pain': -1.0, 'N-acetyl cysteine': 1.0, 'no work': 0.33773659842130571, 'slept late': 1.0, 'hyperventilating': 1.0, 'headache': 1.0, 'Car travel': 1.0, 'loss of appetite': -1.0, 'gluten': 1.0, 'counselling': -1.0, 'relaxed': 0.2546675274400606, 'sleep paralysis': -1.0, 'job interview': -1.0, 'gum pain': 1.0, 'leg cramps': 1.0, 'worked': -0.99999999999999989, 'cramping': -1.0, 'hot and humid': 1.0, 'anxiety': -0.67908157363105204, 'late night': 1.0, 'gassy': 1.0, 'depressed': -1.0, 'work': -0.028502515518841847, 'Alcohol previous night': -1.0, 'black dogs': -0.99999999999999989, 'Ibuprofen': -1.0, 'body aches': -1.0, 'happy': 0.28376858082346212, 'menstruation': 0.77075261423951758, 'jet lag': 1.0, 'left the house': 1.0, 'No Exercise': -1.0, 'Acetaminophen': -1.0, 'very tired': 1.0, 'sad': 1.0, 'cleaned': 1.0, 'food poisoning': 0.99999999999999978, 'junk food': 1.0, 'Good day': 0.078845643646479585, 'trouble getting up': 0.74428827216768367, 'productive': 0.87982361284411625, 'yoga': 1.0, 'choking': -0.99999999999999989, 'migraine': 0.85801944407584452, 'travel': 0.94284913443910523, 'nap': 1.0, 'sick': 0.97035396428507192, 'panic attack': 0.99999999999999978, 'feet tingly': 1.0, 'itchy eyes': 1.0, 'pain': 1.0, 'shoulder ache': 1.0, 'back pain': 0.98246264138964201, 'menstruating': 0.99999999999999978, 'Daytime nap': 0.99999999999999989, 'fatigue': -1.0, 'cleaning': 0.99999999999999978, 'no breakfast': 1.0, 'Mebeverine': 0.99999999999999978, 'good tired': 1.0, 'long day': 1.0, 'Anxious': 0.90960676992380352, 'shortness of breath': -1.0, 'hungry': -0.99999999999999989, 'walked': 0.5243943388209833, 'cooked': 1.0, 'forgot one dose of meds': 0.65271861757395488, 'tooth ache': -1.0, 'Irritable': -0.75869499179916911}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "#set gives us a list of all distinct users, as well as a shuffle\n",
    "good_sleep_users = list(set(df[(df['treatment'] == \"good sleep\") & (df['condition'] == \"Depression\")]['user_id']))\n",
    "test_users = good_sleep_users[:6]  #grab %20 of users for testing\n",
    "train_users = good_sleep_users[6:]\n",
    "train_rows = df[df['user_id'].isin(train_users)]\n",
    "test_rows = df[df['user_id'].isin(test_users)]\n",
    "\n",
    "#just going to abstract this now in case we need it later\n",
    "#finds the pearson correlation between the specified treatment, and all of the treatments used to in conjunction with the specified condition\n",
    "def correlate_treatments(train_df, treatment, condition):\n",
    "    affected_rows = train_df[train_df['condition'] == condition]\n",
    "    other_treatments = list(set(affected_rows[affected_rows['treatment'] != treatment]['treatment']))\n",
    "    treatment_correlations = {}\n",
    "    for treatment2 in other_treatments:\n",
    "        users_with_treatment = list(set(affected_rows[affected_rows['treatment'] == treatment2]['user_id']))\n",
    "        treatment1_values = affected_rows[(affected_rows['user_id'].isin(users_with_treatment)) & (affected_rows['treatment'] == treatment)]['effectiveness']\n",
    "        treatment2_values = affected_rows[(affected_rows['user_id'].isin(users_with_treatment)) & (affected_rows['treatment'] == treatment2)]['effectiveness']\n",
    "        if len(treatment1_values) > 1 :\n",
    "            correlation = pearsonr(treatment1_values, treatment2_values)[0]\n",
    "            treatment_correlations[treatment2] = correlation\n",
    "    return treatment_correlations\n",
    "        \n",
    "treatment_correlations = correlate_treatments(train_rows, 'good sleep', 'Depression')\n",
    "print treatment_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that the number of correlations we've learned between \"good sleep\" and other treatments for depression is very low.  \n",
    "The reason for this is that if we look at users that have tried \"good sleep\" and any other tag, they are almost always the only user that has tried that combination.  Which leaves us comparing single values, which Pearson can't help us with.  We can also see a lot of correlations that are basically 1 or -1, which are usually occurring when we have just two users with the same tags.\n",
    "\n",
    "This may still be the best way to build the recommender system, but the volume of data would need to increase, probably by orders of magnitude.\n",
    "\n",
    "We can still try a different distance measure, so let's try the cosine similarity.  This way we can still get the distance between two points.  Whether those distances will be useful remains to be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cosine_distances(train_df, treatment, condition):\n",
    "    affected_rows = train_df[train_df['condition'] == condition]\n",
    "    other_treatments = list(set(affected_rows[affected_rows['treatment'] != treatment]['treatment']))\n",
    "    treatment_correlations = {}\n",
    "    for treatment2 in other_treatments:\n",
    "        users_with_treatment = list(set(affected_rows[affected_rows['treatment'] == treatment2]['user_id']))\n",
    "        treatment1_values = affected_rows[(affected_rows['user_id'].isin(users_with_treatment)) & (affected_rows['treatment'] == treatment)]['effectiveness']\n",
    "        treatment2_values = affected_rows[(affected_rows['user_id'].isin(users_with_treatment)) & (affected_rows['treatment'] == treatment2)]['effectiveness']\n",
    "        if not np.isnan(treatment2_values).any():\n",
    "            cos_distance = distance.cosine(treatment1_values, treatment2_values)\n",
    "            treatment_correlations[treatment2] = cos_distance\n",
    "    return treatment_correlations\n",
    "\n",
    "treatment_correlations = cosine_distances(train_rows, 'good sleep', 'Depression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is a bit long to print out.  There are a lot of tags, but a lot of them have a distance of 0.   Still, if one of the 0 length tags is found in our test set these entries might be useful.  It's the equivalent of saying \"this treatment worked this well for this one other person\".  So maybe better than nothing.\n",
    "\n",
    "I will now perform a test and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value most correlated to good sleep was happy with value 0.563930772324\n",
      "value most correlated to good sleep was tired with value 0.635950377672\n",
      "value most correlated to good sleep was couch potato with value 2.0\n",
      "value most correlated to good sleep was happy with value 0.563930772324\n",
      "value most correlated to good sleep was cleaning with value 0.366664212496\n",
      "value most correlated to good sleep was walked with value 0.385388498727\n",
      "predicted value was is 0.75 real value is 0.222222222222\n",
      "predicted value was is 0.0 real value is -1.0\n",
      "predicted value was is -0.0454545454545 real value is 0.2\n",
      "predicted value was is -0.8 real value is -0.8\n",
      "predicted value was is -0.666666666667 real value is -1.41666666667\n",
      "predicted value was is 1.16666666667 real value is 1.16666666667\n",
      "r2 accuracy score 0.591630696131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#takes all of the treatments that a user has tried, and finds which one is most correlated to \"good sleep\"\n",
    "def predict_effectiveness(x):\n",
    "    highestCorrelationValue = 0\n",
    "    highestCorrelationKey = \"\"\n",
    "    highestCorrelationEffectiveness = 0\n",
    "    for value in x:\n",
    "        if value in treatment_correlations.keys():\n",
    "            if treatment_correlations[value] > highestCorrelationValue:\n",
    "                highestCorrelationValue = treatment_correlations[value]\n",
    "                highestCorrelationKey = value\n",
    "                highestCorrelationEffectiveness = treatment_correlations\n",
    "    print \"value most correlated to good sleep was \" + highestCorrelationKey + \" with value \" + str(highestCorrelationValue)\n",
    "    return str(highestCorrelationValue) + \",\" + highestCorrelationKey\n",
    "\n",
    "test_depression_rows = test_rows[test_rows['condition'] == 'Depression']\n",
    "test_depression_rows['closest_correlation'] = test_depression_rows.groupby('user_id')['treatment'].transform(predict_effectiveness)\n",
    "real_values = []\n",
    "predicted_values = []\n",
    "for user in test_users:\n",
    "    corr_value,corr_key = test_depression_rows[(test_depression_rows['user_id'] == user) & (test_depression_rows['treatment'] == 'good sleep')]['closest_correlation'].values[0].split(',')\n",
    "    predicted_value = test_depression_rows[(test_depression_rows['user_id'] == user) & (test_depression_rows['treatment'] == corr_key)]['effectiveness'].values[0]\n",
    "    real_value = test_depression_rows[(test_depression_rows['user_id'] == user)]['effectiveness'].values[0]\n",
    "    print \"predicted value is \" + str(predicted_value) + \" real value is \" + str(real_value)\n",
    "    real_values.append(real_value)\n",
    "    predicted_values.append(predicted_value)\n",
    "print \"r2 accuracy score \" + str(r2_score(real_values, predicted_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Based Collaborative Filtering with KMeans\n",
    "\n",
    "Initial results on the item based filter are not exactly ready from production, but still encouraging.  It's showing some predictive power, and on very little data.  Lets not get to excited though, due to the very small test set size.\n",
    "\n",
    "Lets try the other approach.  This way of building a collaborative filter uses that strategy of finding a user's neighbours.  This can be done either with K-Means or with K-Nearest Neighbours.  Our clustering analysis shows that either of these options will make good candidates, so we are going to have to try both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pivot_to_per_user(df):\n",
    "    user_df = df.pivot_table(index='user_id',columns='treatment',values='effectiveness').reset_index()\n",
    "    user_df = user_df.fillna(0)\n",
    "    return user_df.drop('user_id', axis=1)\n",
    "\n",
    "all_good_sleep = pivot_to_per_user(df[df['user_id'].isin(good_sleep_users)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 accuracy score with num_clusters=3 pca_components=100 : -0.00387512439408\n",
      "r2 accuracy score with num_clusters=3 pca_components=200 : -0.0421293170453\n",
      "r2 accuracy score with num_clusters=5 pca_components=100 : 0.390109931796\n",
      "r2 accuracy score with num_clusters=5 pca_components=200 : -0.0140262789178\n",
      "r2 accuracy score with num_clusters=10 pca_components=100 : -0.0437776020312\n",
      "r2 accuracy score with num_clusters=10 pca_components=200 : -0.0437776020312\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#PCA, then K-means, then find the mean effectiveness for the target treatment for each cluster\n",
    "#data is the df containing both your train and test sets\n",
    "#split is how many users you want split into the test set\n",
    "#pca_components and num_clusters are hyperparameters\n",
    "#treatment is the target treatment\n",
    "def find_cluster_effectiveness(data, split, pca_components, num_clusters, treatment):\n",
    "    #applying PCA because the number of features in the pivoted dataset is very high\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    pca.fit(data)\n",
    "    fit_data = pca.transform(all_good_sleep)\n",
    "    train = fit_data[split:]\n",
    "    test = fit_data[:split]\n",
    "    test_df = all_good_sleep.iloc[:split,:] #splitting on same lines so I can look these users up at test time\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(train)\n",
    "    pred_train = kmeans.predict(train)\n",
    "    pred_test = kmeans.predict(test)\n",
    "\n",
    "    #now look up which cluster each test sample falls into, and take the mean \"good_sleep\" effectiveness for that cluster\n",
    "    cluster_effectivness = []\n",
    "    for i in range(len(pred_train)):\n",
    "        rows_in_cluster = all_good_sleep.iloc[np.where(pred_train == i)[0],:][treatment]\n",
    "        if len(rows_in_cluster) != 0:\n",
    "            cluster_effectivness.append(np.mean(rows_in_cluster))\n",
    "        else:\n",
    "            #print \"no value for this cluster\"\n",
    "            cluster_effectivness.append(0) #default to 0 I guess?  Not sure about this.  Pretty rare anyways.\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    for i in range(len(pred_test)):\n",
    "        predicted.append(cluster_effectivness[pred_test[i]])\n",
    "        actual.append(test_df.iloc[i,:][treatment])\n",
    "        #print \"predicted value is \" + str(cluster_effectivness[pred_test[i]]) + \" actual value is \" + str(test_df.iloc[i,:][treatment])\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    return r2\n",
    "\n",
    "#search to find best PCA components and num clusters\n",
    "num_clusters_list = [3, 5, 10]\n",
    "pca_components_list = [100, 200]\n",
    "for num_clusters in num_clusters_list:\n",
    "    for pca_components in pca_components_list:\n",
    "        r2 = find_cluster_effectiveness(all_good_sleep, num_clusters, pca_components,15,\"good sleep\")\n",
    "        print \"r2 accuracy score with num_clusters=\" + str(num_clusters) + \" pca_components=\" +str(pca_components) + \" : \"+ str(r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values at 5 clusters and 100 pca_components is consistently the best r2, but all of the r2 scores vary wildly between runs.  If this methodology is used going forward we will need to take this into consideration.  This bit of hyperparameter optimization runs fast, but will need to be run for every treatment every time the model is built.  We will also need to run kmeans several times and measure the siloutte scores, as they are coming out very random and having a large impact on the r2 accuracy scores.  This can be observed by running the code block above several times, and you will see that the score jump around rather a lot.\n",
    "\n",
    "### User Based Collaborative Filtering with KMeans\n",
    "\n",
    "This is finding the K users that are most like the test user, and taking the mean of their target values.  Fortunately for me, this is exactly what scikit-learn's Nearest Neighbour Regressor does.  And this time I'll be able to use exhaustive grid search to find my parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 5, 'weights': 'distance'}\n",
      "[-0.09248827 -0.09647474 -0.07091287  0.05764681 -0.09232114 -0.09338541\n",
      " -0.04368059]\n",
      "-0.288616202803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import grid_search\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#using same data structure as kmeans solution\n",
    "X = all_good_sleep.drop('good sleep', axis=1)\n",
    "y = all_good_sleep['good sleep']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "parameters = {'weights':('uniform', 'distance'), 'n_neighbors':[1,2,3,4,5,6,7,8]}\n",
    "nn = KNeighborsRegressor(1)\n",
    "clf = grid_search.GridSearchCV(nn, parameters)  #scorer defaults to r2\n",
    "clf.fit(X_train, y_train)\n",
    "print clf.best_params_\n",
    "pred = clf.predict(X_test)\n",
    "print pred\n",
    "print r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch.  We might be able to improve that with PCA.  But given the extremely low score, it doesn't seem like a valuable use of time to continue this one.\n",
    "\n",
    "\n",
    "### Wrap-up\n",
    "\n",
    "The item based recommender is greatly outperforming both types of user based recommenders.  For the time being I'll be proceeding with building that out.  It should be noted though, that as this dataset grows in both breadth and depth, we should revisit this notebook.  Just drop in a new CSV and run each section and see how the r2 scores change. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
